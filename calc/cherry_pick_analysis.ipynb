{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad7b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_alignment_scores, calculate_novelty_scores, to_the_forms, calculate_f_scores\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "066645d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\Valentin\\Desktop\\data_for_impact_analysis\\data\\aq\\outputs\\selt.txt_llama_t0.3_cyc1.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop([\"Reasoning\", \"Cycle\"], axis=1)\n",
    "df.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51645061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"C:\\\\Users\\\\Valentin\\\\Desktop\\\\data_for_impact_analysis\\\\data\\\\aq\\\\outputs\\\\selt.txt_llama_t0.3_cyc1.csv\": {\n",
      "        \"combined\": [\n",
      "            0.74,\n",
      "            0.14,\n",
      "            0.69,\n",
      "            [\n",
      "                0.75,\n",
      "                0.6666666666666666,\n",
      "                0.6666666666666666,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                0.5,\n",
      "                0.375,\n",
      "                1.0\n",
      "            ]\n",
      "        ],\n",
      "        \"orange_nikita\": [\n",
      "            0.76,\n",
      "            0.25,\n",
      "            0.57,\n",
      "            [\n",
      "                1.0,\n",
      "                0.0,\n",
      "                0.6666666666666666,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                0.4,\n",
      "                1.0,\n",
      "                1.0\n",
      "            ]\n",
      "        ],\n",
      "        \"orange_sasha\": [\n",
      "            0.93,\n",
      "            0.27,\n",
      "            0.55,\n",
      "            [\n",
      "                1.0,\n",
      "                0.8,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                0.6666666666666666,\n",
      "                1.0\n",
      "            ]\n",
      "        ],\n",
      "        \"orange_valya\": [\n",
      "            0.8,\n",
      "            0.33,\n",
      "            0.39,\n",
      "            [\n",
      "                1.0,\n",
      "                0.5,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                0.5,\n",
      "                0.4,\n",
      "                1.0\n",
      "            ]\n",
      "        ],\n",
      "        \"orange_vanya\": [\n",
      "            0.85,\n",
      "            0.3,\n",
      "            0.48,\n",
      "            [\n",
      "                0.6666666666666666,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                0.6666666666666666,\n",
      "                0.5,\n",
      "                1.0\n",
      "            ]\n",
      "        ],\n",
      "        \"average_alignment\": 0.82,\n",
      "        \"average_novelty\": 0.26,\n",
      "        \"average_f_score\": 0.54\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_PATH = r\"C:\\Users\\Valentin\\Desktop\\data_for_impact_analysis\\data\\aq\"\n",
    "\n",
    "CHANGE_REQUESTS_PATH = os.path.join(PROJECT_PATH, \"change_requests.txt\")\n",
    "OUTPUTS_DIR = os.path.join(PROJECT_PATH, \"outputs\")\n",
    "TEAM_DATA = os.path.join(PROJECT_PATH, \"team_data\")\n",
    "relevancy = 0.46\n",
    "\n",
    "\n",
    "def calculate_all_team():\n",
    "    results = {}\n",
    "    # List all output files\n",
    "    output_files = [os.path.join(OUTPUTS_DIR,r\"selt.txt_llama_t0.3_cyc1.csv\")]\n",
    "    # List all participant files (exclude _survey)\n",
    "    participant_files = [f for f in os.listdir(TEAM_DATA) if not f.endswith('_survey.txt') and f.endswith('.txt')]\n",
    "\n",
    "    for output_file in output_files:\n",
    "        output_path = os.path.join(OUTPUTS_DIR, output_file)\n",
    "        df1 = pd.read_csv(output_path)\n",
    "        df1['Relevant']= 'Yes'\n",
    "        participant_scores = {}\n",
    "        alig_scores_list = []\n",
    "        avg_align_scores = []\n",
    "        avg_nov_scores = []\n",
    "        avg_f_scores = []\n",
    "        for participant_file in participant_files:\n",
    "            participant_path = os.path.join(TEAM_DATA, participant_file)\n",
    "            df2 = pd.read_csv(participant_path)\n",
    "            scores, avg_score = calculate_alignment_scores(CHANGE_REQUESTS_PATH, df1, df2)\n",
    "            scores2, avg_score2 = calculate_novelty_scores(df1,df2)\n",
    "            scores3, avg_score3, _ = calculate_f_scores(CHANGE_REQUESTS_PATH,df1,df2)\n",
    "            avg_score2  = avg_score2 * relevancy\n",
    "            participant_name = participant_file.replace('.txt', '')\n",
    "            participant_scores[participant_name] = (round(avg_score,2), round(avg_score2,2), round(avg_score3,2),scores )\n",
    "            avg_align_scores.append(avg_score)\n",
    "            avg_nov_scores.append(avg_score2)\n",
    "            avg_f_scores.append(avg_score3)\n",
    "\n",
    "        # Add average for the team\n",
    "        participant_scores['average_alignment'] = round(sum(avg_align_scores) / len(avg_align_scores),2) if avg_align_scores else 0\n",
    "        participant_scores['average_novelty'] = round(sum(avg_nov_scores) / len(avg_nov_scores),2) if avg_nov_scores else 0\n",
    "        participant_scores['average_f_score'] = round(sum(avg_f_scores) / len(avg_f_scores),2) if avg_f_scores else 0\n",
    "\n",
    "\n",
    "        results[output_file] = participant_scores\n",
    "    return results\n",
    "\n",
    "all_scores = calculate_all_team()\n",
    "print(json.dumps(all_scores, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7cb178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior > Middle: p-value = 0.2292\n",
      "❌ Not statistically significant\n",
      "\n",
      "Middle > Junior: p-value = 0.6328\n",
      "❌ Not statistically significant\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Data (flattened lists)\n",
    "senior = [\n",
    "    0.4, 0.3333333333333333, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 0.0,  # Optics\n",
    "    0.3333333333333333, 0.75, 0.8, 0.4, 0.8333333333333334, 0.3, 0.5, 0.5, 0.6666666666666666, 0.5, 0.8, 1.0, 1.0, 0.8, 0.2, 0.5, 0.5, 0.75,  # Sensors\n",
    "    1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0  # Orange\n",
    "]\n",
    "\n",
    "middle = [\n",
    "    0.4, 0.4, 0.5, 1.0, 0.16666666666666666, 0.4, 0.6666666666666666, 0.0, 0.25, 0.5, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0,  # Optics\n",
    "    0.5, 0.75, 1.0, 1.0, 0.75, 0.25, 0.5714285714285714, 0.5, 0.6666666666666666,  # Sensors\n",
    "    1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0  # Orange\n",
    "]\n",
    "\n",
    "junior = [\n",
    "    0.5, 0.4, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0,  # Optics\n",
    "    1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0  # Orange\n",
    "]\n",
    "\n",
    "# Perform Mann-Whitney U tests\n",
    "# Senior > Middle?\n",
    "stat_senior_middle, p_senior_middle = mannwhitneyu(senior, middle, alternative='greater')\n",
    "print(f\"Senior > Middle: p-value = {p_senior_middle:.4f}\")\n",
    "if p_senior_middle < 0.05:\n",
    "    print(\"✅ Statistically significant (Senior > Middle)\")\n",
    "else:\n",
    "    print(\"❌ Not statistically significant\")\n",
    "\n",
    "# Middle > Junior?\n",
    "stat_middle_junior, p_middle_junior = mannwhitneyu(middle, junior, alternative='greater')\n",
    "print(f\"\\nMiddle > Junior: p-value = {p_middle_junior:.4f}\")\n",
    "if p_middle_junior < 0.05:\n",
    "    print(\"✅ Statistically significant (Middle > Junior)\")\n",
    "else:\n",
    "    print(\"❌ Not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3141b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior > Middle: p-value = 0.6636\n",
      "❌ NOT SIGNIFICANT: No evidence that Senior > Middle\n",
      "\n",
      "Middle > Junior: p-value = 0.2358\n",
      "❌ NOT SIGNIFICANT: No evidence that Middle > Junior\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Raw data (manually parsed)\n",
    "senior_scores = [\n",
    "    0.4, 0.3333333333333333, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 0.0,  # Optics\n",
    "    0.3333333333333333, 0.75, 0.8, 0.4, 0.8333333333333334, 0.3, 0.5, 0.5, 0.6666666666666666,  # Sensors\n",
    "    0.5, 0.8, 1.0, 1.0, 0.8, 0.2, 0.5, 0.5, 0.75,  # Sensors (continued)\n",
    "    1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0  # Orange\n",
    "]\n",
    "\n",
    "middle_scores = [\n",
    "    0.4, 0.4, 0.5, 1.0, 0.16666666666666666, 0.4, 0.6666666666666666, 0.0,  # Optics\n",
    "    0.25, 0.5, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0,  # Optics (continued)\n",
    "    0.5, 0.75, 1.0, 1.0, 0.75, 0.25, 0.5714285714285714, 0.5, 0.6666666666666666,  # Sensors\n",
    "    1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0  # Orange\n",
    "]\n",
    "\n",
    "junior_scores = [\n",
    "    0.5, 0.4, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0,  # Optics\n",
    "    1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0  # Orange\n",
    "]\n",
    "\n",
    "# Normalize scores by team average\n",
    "def normalize_scores(scores):\n",
    "    team_avg = np.mean(scores)\n",
    "    return [x / team_avg for x in scores]\n",
    "\n",
    "senior_normalized = normalize_scores(senior_scores)\n",
    "middle_normalized = normalize_scores(middle_scores)\n",
    "junior_normalized = normalize_scores(junior_scores)\n",
    "\n",
    "# Perform Mann-Whitney U test (one-tailed: greater)\n",
    "def test_greater(group1, group2, group1_name, group2_name):\n",
    "    stat, p = mannwhitneyu(group1, group2, alternative='greater')\n",
    "    print(f\"{group1_name} > {group2_name}: p-value = {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(f\"✅ SIGNIFICANT: {group1_name} scores are higher than {group2_name}\")\n",
    "    else:\n",
    "        print(f\"❌ NOT SIGNIFICANT: No evidence that {group1_name} > {group2_name}\")\n",
    "    print()\n",
    "\n",
    "# Test senior > middle > junior\n",
    "test_greater(senior_normalized, middle_normalized, \"Senior\", \"Middle\")\n",
    "test_greater(middle_normalized, junior_normalized, \"Middle\", \"Junior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fb9b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128 unique components after deduplication\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_component_name(name):\n",
    "    \"\"\"Standardize component names for better deduplication\"\"\"\n",
    "    return str(name).strip().lower()\n",
    "\n",
    "def combine_and_deduplicate(team_data_folder):\n",
    "    # Get all participant files (excluding survey files)\n",
    "    participant_files = [\n",
    "        f for f in os.listdir(team_data_folder) \n",
    "        if not f.endswith('_survey.txt') and f.endswith('.txt')\n",
    "    ]\n",
    "    \n",
    "    # Initialize empty DataFrame\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    # Process each file\n",
    "    for file in participant_files:\n",
    "        file_path = os.path.join(team_data_folder, file)\n",
    "        try:\n",
    "            # Read file (adjust separator if needed)\n",
    "            df = pd.read_csv(file_path, sep=',')\n",
    "            \n",
    "            # Standardize column names\n",
    "            \n",
    "            \n",
    "            # Ensure required columns exist\n",
    "            if {'Component', 'Change'}.issubset(df.columns):\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    if combined_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Clean component names for consistent comparison\n",
    "    combined_df['component_clean'] = combined_df['Component'].apply(clean_component_name)\n",
    "    \n",
    "    # Drop duplicates based on cleaned component name and change value\n",
    "    # Keep first occurrence, case insensitive\n",
    "    deduplicated_df = combined_df.drop_duplicates(\n",
    "        subset=['component_clean', 'Change'],\n",
    "        keep='first'\n",
    "    ).drop(columns=['component_clean'])\n",
    "    \n",
    "    # Reset index after deduplication\n",
    "    deduplicated_df.reset_index(drop=True, inplace=True)\n",
    "    deduplicated_df.groupby(\"Change\")\n",
    "    \n",
    "    return deduplicated_df\n",
    "TEAM_DATA = r\"C:\\Users\\Valentin\\Desktop\\data_for_impact_analysis\\data\\inTouch\\team_data\"\n",
    "# Usage example:\n",
    "result_df = combine_and_deduplicate(TEAM_DATA)\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(result_df)} unique components after deduplication\")\n",
    "result_df.to_csv(os.path.join(TEAM_DATA, \"combined.txt\"),index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d82de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'')\n",
    "to_the_forms(df1, change_requests_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7518e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 - LLMs response, df2 - human response\n",
    "df1 = pd.read_csv(r'C:\\Users\\Valentin\\Desktop\\data_for_impact_analysis\\data\\aq\\outputs\\selt.txt_llama_t0.3_cyc1.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\Valentin\\Desktop\\data_for_impact_analysis\\data\\aq\\team_data\\orange_valya.txt')\n",
    "df1['Relevant'] = \"Yes\"\n",
    "relevancy = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83745663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Angel_survey.txt\": 0.5416666666666666,\n",
      "    \"EgorK_survey.txt\": 0.3333333333333333,\n",
      "    \"IlonaP_survey.txt\": 0.2708333333333333,\n",
      "    \"Ivan_survey.txt\": 0.5833333333333334\n",
      "}\n",
      "/n average: 0.43229166666666663\n"
     ]
    }
   ],
   "source": [
    "participant_files = [f for f in os.listdir(TEAM_DATA) if f.endswith('_survey.txt') and f.endswith('.txt')]\n",
    "\n",
    "participant_relevancy_score  = {}\n",
    "avg = []\n",
    "\n",
    "\n",
    "for file in participant_files:\n",
    "    file = os.path.join(TEAM_DATA, file)\n",
    "    df_n = pd.read_csv(filepath_or_buffer=file,sep = '\\t', header = None)\n",
    "    df_n = df_n.drop(df_n.columns[[0,1]], axis=1).T  # Drop columns at positions 0, 1\n",
    "    df_n.columns = ['Relevant']\n",
    "    relevancy = len(df_n[df_n[\"Relevant\"] == \"Yes\"])/len(df_n)\n",
    "    participant_relevancy_score[os.path.basename(file)] = relevancy\n",
    "    avg.append(relevancy)\n",
    "\n",
    "avg = sum(avg)/len(avg)\n",
    "print(json.dumps(participant_relevancy_score, indent=4))\n",
    "print(f\"/n average: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53952c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg alignment: 0.55\n"
     ]
    }
   ],
   "source": [
    "scores, avg_score = calculate_alignment_scores(change_requests_path,df1, df2)\n",
    "print(\"avg alignment:\", round(avg_score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9684c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_n = pd.read_csv(filepath_or_buffer=r'team_data\\nastya_survey.csv',sep = '\\t', header = None)\n",
    "# df_n = df_n.drop(df_n.columns[[0,1]], axis=1).T  # Drop columns at positions 0, 1\n",
    "# df_n.columns = ['Relevant']\n",
    "# df_n.index = range(0, len(df_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f608ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty with relevancy adjustment 0.14\n"
     ]
    }
   ],
   "source": [
    "novelty_scores, nov = calculate_novelty_scores(df1, df2)\n",
    "nov = nov*relevancy \n",
    "print(\"Novelty with relevancy adjustment\", round(nov,2) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
