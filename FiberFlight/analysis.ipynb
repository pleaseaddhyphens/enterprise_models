{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad7b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_prompt_files, run_full_analysis, to_the_forms, read_numbered_txt_files, process_all_change_requests, calculate_alignment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51645061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "model_path = r\"export.json\"\n",
    "change_requests_path = r\"change_requests.txt\"\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "agent_id = \"ag:ac256ded:20250525:impact-research:a7aac9eb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d59059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running cycle 1/1 ===\n",
      "Processing request 1/9...\n",
      "Processing request 2/9...\n",
      "Processing request 3/9...\n",
      "Processing request 4/9...\n",
      "Processing request 5/9...\n",
      "Processing request 6/9...\n",
      "Processing request 7/9...\n",
      "Processing request 8/9...\n",
      "Processing request 9/9...\n"
     ]
    }
   ],
   "source": [
    "df= run_full_analysis(model_path, change_requests_path,1,3,5,api_key,agent_id)\n",
    "to_the_forms(df, change_requests_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc60f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prompt file: to-the-chat\\1.txt\n",
      "Created prompt file: to-the-chat\\2.txt\n",
      "Created prompt file: to-the-chat\\3.txt\n",
      "Created prompt file: to-the-chat\\4.txt\n",
      "Created prompt file: to-the-chat\\5.txt\n",
      "Created prompt file: to-the-chat\\6.txt\n",
      "Created prompt file: to-the-chat\\7.txt\n",
      "Created prompt file: to-the-chat\\8.txt\n",
      "Created prompt file: to-the-chat\\9.txt\n"
     ]
    }
   ],
   "source": [
    "create_prompt_files(change_requests_path, model_path, output_folder='to-the-chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d82de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = read_numbered_txt_files(\"to-the-chat\")\n",
    "# df = process_all_change_requests(model_path, change_requests_path, 1, manual_responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df7518e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 - LLMs response, df2 - human responce\n",
    "df1 = pd.read_csv(r'outputs\\to-the-calc.csv')\n",
    "df2 = pd.read_csv(r'team_data\\nastya.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53952c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666666666666666, 0.0, 0.5, 1.0, 0.375, 0.25, 0.14285714285714285, 0.5, 0.16666666666666666] avg_score: 0.3445767195767196\n"
     ]
    }
   ],
   "source": [
    "# Alignment for Nastya\n",
    "scores, avg_score = calculate_alignment_scores(change_requests_path,df1, df2)\n",
    "print(scores,\"avg_score:\", avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9684c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = pd.read_csv(filepath_or_buffer=r'team_data\\nastya_survey.csv',sep = '\\t', header = None)\n",
    "df_n = df_n.drop(df_n.columns[[0,1]], axis=1).T  # Drop columns at positions 0, 1\n",
    "df_n.columns = ['Relevant']\n",
    "df_n.index = range(0, len(df_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "576a0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevacy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "df1['Relevant'] = df_n['Relevant']\n",
    "relevacy = len(df1[df1[\"Relevant\"] == \"Yes\"])/len(df1)\n",
    "print(\"Relevacy:\", relevacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f608ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty [0.5, 1.0, 0.3333333333333333, 0.0, 0.25, 0.5, 0.0, 0.5, 0.0] avg: 0.3425925925925926\n"
     ]
    }
   ],
   "source": [
    "def calculate_novelty_scores(df1, df2):\n",
    "    \"\"\"\n",
    "    Calculate novelty scores for each change where:\n",
    "    Novelty Score = (Number of new relevant components) / (Total relevant components in df1 for that change)\n",
    "    \n",
    "    Returns a DataFrame with Change and Novelty_Score columns\n",
    "    \"\"\"\n",
    "    # Filter relevant components in df1 (case insensitive)\n",
    "    df1_relevant = df1[df1['Relevant'].str.lower() == 'yes'].copy()\n",
    "    df1_relevant['Component_lower'] = df1_relevant['Component'].str.lower()\n",
    "    \n",
    "    # Prepare df2 components (case insensitive)\n",
    "    df2 = df2.copy()\n",
    "    df2['Component_lower'] = df2['Component'].str.lower()\n",
    "    \n",
    "    # Get common changes between both dataframes\n",
    "    common_changes = set(df1['Change']).intersection(set(df2['Change']))\n",
    "    \n",
    "    novelty_scores = []\n",
    "    \n",
    "    for change in common_changes:\n",
    "        # Get components for this change in both dataframes\n",
    "        df1_components = set(df1_relevant[df1_relevant['Change'] == change]['Component_lower'])\n",
    "        df2_components = set(df2[df2['Change'] == change]['Component_lower'])\n",
    "        \n",
    "        # Calculate novelty metrics\n",
    "        total_components = len(df1_components)\n",
    "        new_components = len(df1_components - df2_components)\n",
    "        \n",
    "        if total_components > 0:  # Avoid division by zero\n",
    "            novelty_score = new_components / total_components\n",
    "        else:\n",
    "            novelty_score = 0.0\n",
    "        \n",
    "        novelty_scores.append(novelty_score)\n",
    "    avg_novelty = sum(novelty_scores)/len(novelty_scores) if novelty_scores else 0\n",
    "    return novelty_scores, avg_novelty\n",
    "\n",
    "# Example usage\n",
    "novelty_scores, avg = calculate_novelty_scores(df1, df2)\n",
    "print(\"Novelty\", novelty_scores, \"avg:\", avg )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
